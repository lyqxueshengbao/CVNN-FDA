"""
FDA-CVNN ç½‘ç»œæ¨¡å‹
ç«¯åˆ°ç«¯å›å½’ï¼šè¾“å…¥åæ–¹å·®çŸ©é˜µï¼Œè¾“å‡ºè·ç¦»å’Œè§’åº¦
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from layers_complex import (
    ComplexConv2d, 
    ComplexBatchNorm2d, 
    ModReLU, 
    ComplexAvgPool2d,
    ComplexAdaptiveAvgPool2d
)
import config as cfg


# ==========================================
# å¤æ•°æ³¨æ„åŠ›æ¨¡å—
# ==========================================
class ComplexSEBlock(nn.Module):
    """
    å¤æ•° Squeeze-and-Excitation (SE) é€šé“æ³¨æ„åŠ›
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. Squeeze: å…¨å±€å¹³å‡æ± åŒ–å‹ç¼©ç©ºé—´ç»´åº¦
    2. Excitation: ä¸¤å±‚ FC å­¦ä¹ é€šé“é—´å…³ç³»
    3. Scale: ç”¨å­¦åˆ°çš„æƒé‡é‡æ–°åŠ æƒå„é€šé“
    
    å¯¹äºå¤æ•°ï¼šä½¿ç”¨æ¨¡å€¼æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œç„¶ååŒæ—¶ç¼©æ”¾å®éƒ¨å’Œè™šéƒ¨
    """
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.avg_pool = ComplexAdaptiveAvgPool2d(1)
        
        # Excitation: ä¸¤å±‚ FC (ä½œç”¨äºæ¨¡å€¼)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        b, _, c, h, w = x.shape
        
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ– -> [B, 2, C, 1, 1]
        y = self.avg_pool(x)
        
        # è®¡ç®—æ¨¡å€¼ä½œä¸ºæ³¨æ„åŠ›è¾“å…¥ -> [B, C]
        real = y[:, 0, :, 0, 0]  # [B, C]
        imag = y[:, 1, :, 0, 0]  # [B, C]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)
        
        # Excitation: å­¦ä¹ é€šé“æƒé‡ -> [B, C]
        attn = self.fc(mag)
        
        # Scale: é‡æ–°åŠ æƒ -> [B, 1, C, 1, 1]
        attn = attn.view(b, 1, c, 1, 1)
        
        return x * attn


class ComplexFARBlock(nn.Module):
    """
    å¤æ•°ç‰ˆ FAR (Feature Attention Refinement) Block
    
    ä¸ SE çš„æ ¸å¿ƒåŒºåˆ«ï¼š
    - SE: å…¨å±€æ± åŒ– â†’ é€šé“çº§æ³¨æ„åŠ› [B, 1, C, 1, 1]
    - FAR: å±€éƒ¨æ± åŒ– â†’ ç©ºé—´+é€šé“çº§æ³¨æ„åŠ› [B, 1, C, H, W]
    
    ä¼˜åŠ¿ï¼šä¿ç•™ç©ºé—´ä½ç½®ä¿¡æ¯ï¼Œæ›´é€‚åˆåæ–¹å·®çŸ©é˜µè¿™ç§ç©ºé—´ç»“æ„æœ‰æ„ä¹‰çš„è¾“å…¥
    """
    def __init__(self, channels, kernel_size=3, reduction=4):
        super().__init__()
        
        features = max(channels // reduction, 8)  # ç¡®ä¿è‡³å°‘8ä¸ªç‰¹å¾
        padding = (kernel_size - 1) // 2
        
        # 1. å±€éƒ¨å¹³å‡æ± åŒ– (LAP) - è·å–å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä¸æ”¹å˜å°ºå¯¸
        self.local_avg_pool = ComplexAvgPool2d(
            kernel_size=kernel_size, stride=1, padding=padding
        )
        
        # 2. ç‰¹å¾é‡åŠ æƒç½‘ç»œ
        # Layer 1: é™ç»´ (1x1 Conv)
        self.conv1 = ComplexConv2d(channels, features, kernel_size=1)
        self.bn1 = ComplexBatchNorm2d(features)
        self.act1 = ModReLU(features, bias_init=-0.5)
        
        # Layer 2: å‡ç»´ (1x1 Conv)
        self.conv2 = ComplexConv2d(features, channels, kernel_size=1)
        
        # Sigmoid ç”¨äºç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        # 1. å±€éƒ¨å¹³å‡æ± åŒ–è·å–ä¸Šä¸‹æ–‡
        y = self.local_avg_pool(x)  # [B, 2, C, H, W]
        
        # 2. ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act1(y)
        y = self.conv2(y)  # [B, 2, C, H, W]
        
        # 3. åŸºäºæ¨¡å€¼ç”Ÿæˆæ³¨æ„åŠ›å›¾
        real = y[:, 0]  # [B, C, H, W]
        imag = y[:, 1]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)
        attn = self.sigmoid(mag)  # [B, C, H, W]
        
        # æ‰©å±•ç»´åº¦: [B, 1, C, H, W]
        attn = attn.unsqueeze(1)
        
        # 4. é‡åŠ æƒ
        return x * attn


class ComplexCBAM(nn.Module):
    """
    å¤æ•° CBAM (Convolutional Block Attention Module)
    = é€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›
    
    åœ¨ä½ SNR ä¸‹ï¼Œç©ºé—´æ³¨æ„åŠ›å¯ä»¥èšç„¦äºåæ–¹å·®çŸ©é˜µä¸­çš„å…³é”®åŒºåŸŸ
    """
    def __init__(self, channels, reduction=4):
        super().__init__()
        
        # é€šé“æ³¨æ„åŠ› (SE)
        self.channel_attn = ComplexSEBlock(channels, reduction)
        
        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        # 1. é€šé“æ³¨æ„åŠ›
        x = self.channel_attn(x)
        
        # 2. ç©ºé—´æ³¨æ„åŠ›
        # æ²¿é€šé“ç»´åº¦å–å¹³å‡å’Œæœ€å¤§çš„æ¨¡å€¼
        real = x[:, 0]  # [B, C, H, W]
        imag = x[:, 1]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)  # [B, C, H, W]
        
        # é€šé“ç»´åº¦çš„å¹³å‡å’Œæœ€å¤§
        avg_mag = mag.mean(dim=1, keepdim=True)  # [B, 1, H, W]
        max_mag = mag.max(dim=1, keepdim=True)[0]  # [B, 1, H, W]
        
        # æ‹¼æ¥å¹¶ç”Ÿæˆç©ºé—´æ³¨æ„åŠ›å›¾
        spatial_input = torch.cat([avg_mag, max_mag], dim=1)  # [B, 2, H, W]
        spatial_attn = self.spatial_conv(spatial_input)  # [B, 1, H, W]
        spatial_attn = spatial_attn.unsqueeze(1)  # [B, 1, 1, H, W]
        
        return x * spatial_attn


class FDA_CVNN(nn.Module):
    """
    FDA-MIMO å¤æ•°å·ç§¯ç¥ç»ç½‘ç»œ
    
    è¾“å…¥: [Batch, 2, 100, 100] - åæ–¹å·®çŸ©é˜µ (å®éƒ¨é€šé“, è™šéƒ¨é€šé“)
    è¾“å‡º: [Batch, 2] - å½’ä¸€åŒ–çš„ (è·ç¦», è§’åº¦)
    
    æ¶æ„ç‰¹ç‚¹:
    1. ä½¿ç”¨å¤æ•°å·ç§¯ä¿æŒç›¸ä½ä¿¡æ¯
    2. ModReLUæ¿€æ´»å‡½æ•° (è´Ÿåç½®åˆ›é€ éçº¿æ€§)
    3. å¹³å‡æ± åŒ– (ä¸ç ´åç›¸ä½)
    """
    def __init__(self):
        super().__init__()
        
        # è¾“å…¥: [B, 2, 1, 100, 100] -> éœ€è¦è°ƒæ•´ä¸º [B, 2, 1, H, W]
        # é€šé“æ•°ç¿»å€: 32 -> 64 -> 128ï¼Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 5
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.pool3 = ComplexAvgPool2d(5)
        
        # å…¨è¿æ¥å±‚
        # ç‰¹å¾å›¾å¤§å°: 5x5, é€šé“128, å®éƒ¨+è™šéƒ¨
        self.fc_in_dim = 128 * 5 * 5 * 2  # 6400
        
        self.fc1 = nn.Linear(self.fc_in_dim, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 2)  # è¾“å‡º r å’Œ theta
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100] - å®éƒ¨å’Œè™šéƒ¨
        """
        # è°ƒæ•´ç»´åº¦: [B, 2, H, W] -> [B, 2, 1, H, W]
        x = x.unsqueeze(2)
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.pool1(x)  # [B, 2, 16, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.pool2(x)  # [B, 2, 32, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.pool3(x)  # [B, 2, 64, 5, 5]
        
        # å±•å¹³: å°†å¤æ•°ç»´åº¦å’Œç©ºé—´ç»´åº¦åˆå¹¶
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 2*64*5*5]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        return x
    
    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_Attention(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ FDA-CVNN (å¢å¼ºä½SNRæ€§èƒ½)
    
    æ”¹è¿›ç‚¹:
    1. æ¯ä¸ªå·ç§¯å—ååŠ å…¥ SE é€šé“æ³¨æ„åŠ›
    2. æ®‹å·®è¿æ¥å¸®åŠ©æ¢¯åº¦æµåŠ¨
    3. æ›´æ·±çš„ç½‘ç»œç»“æ„
    
    åœ¨ä½ SNR ä¸‹ï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥:
    - è‡ªé€‚åº”æ”¾å¤§åŒ…å«ä¿¡å·ç‰¹å¾çš„é€šé“
    - æŠ‘åˆ¶å™ªå£°ä¸»å¯¼çš„é€šé“
    """
    def __init__(self, use_cbam=False):
        super().__init__()
        self.use_cbam = use_cbam
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        self.attn1 = ComplexCBAM(32) if use_cbam else ComplexSEBlock(32)
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.attn2 = ComplexCBAM(64) if use_cbam else ComplexSEBlock(64)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 12
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.attn3 = ComplexCBAM(128) if use_cbam else ComplexSEBlock(128)
        self.pool3 = ComplexAvgPool2d(2)
        
        # Block 4: 12 -> 6 (æ–°å¢ä¸€å±‚ï¼Œæ›´æ·±çš„ç½‘ç»œ)
        self.conv4 = ComplexConv2d(128, 256, kernel_size=3, padding=1)
        self.bn4 = ComplexBatchNorm2d(256)
        self.act4 = ModReLU(256, bias_init=-0.5)
        self.attn4 = ComplexCBAM(256) if use_cbam else ComplexSEBlock(256)
        self.pool4 = ComplexAvgPool2d(2)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = ComplexAdaptiveAvgPool2d(1)
        
        # å…¨è¿æ¥å±‚
        self.fc_in_dim = 256 * 2  # 256é€šé“ * å®éƒ¨è™šéƒ¨
        
        self.fc1 = nn.Linear(self.fc_in_dim, 256)
        self.fc2 = nn.Linear(256, 64)
        self.fc3 = nn.Linear(64, 2)
        
        self.dropout = nn.Dropout(0.4)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100]
        """
        x = x.unsqueeze(2)  # [B, 2, 1, 100, 100]
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.attn1(x)  # æ³¨æ„åŠ›
        x = self.pool1(x)  # [B, 2, 32, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.attn2(x)
        x = self.pool2(x)  # [B, 2, 64, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.attn3(x)
        x = self.pool3(x)  # [B, 2, 128, 12, 12]
        
        # Block 4
        x = self.conv4(x)
        x = self.bn4(x)
        x = self.act4(x)
        x = self.attn4(x)
        x = self.pool4(x)  # [B, 2, 256, 6, 6]
        
        # å…¨å±€æ± åŒ–
        x = self.global_pool(x)  # [B, 2, 256, 1, 1]
        
        # å±•å¹³
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 512]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc3(x))
        
        return x
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_FAR(nn.Module):
    """
    å¸¦ FAR (Feature Attention Refinement) æ³¨æ„åŠ›çš„ FDA-CVNN
    
    FAR çš„ä¼˜åŠ¿ (ç›¸æ¯” SE):
    1. ä½¿ç”¨å±€éƒ¨å¹³å‡æ± åŒ–ï¼Œä¿ç•™ç©ºé—´ä½ç½®ä¿¡æ¯
    2. ç”Ÿæˆç©ºé—´+é€šé“çº§æ³¨æ„åŠ›å›¾ [B, 1, C, H, W]
    3. æ›´é€‚åˆåæ–¹å·®çŸ©é˜µè¿™ç§ç©ºé—´ç»“æ„æœ‰æ„ä¹‰çš„è¾“å…¥
    4. åœ¨ä½ SNR ä¸‹å¯ä»¥é’ˆå¯¹ä¸åŒä½ç½®ç»™ä¸åŒæƒé‡
    """
    def __init__(self, far_kernel_size=3):
        super().__init__()
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        self.attn1 = ComplexFARBlock(32, kernel_size=far_kernel_size)
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.attn2 = ComplexFARBlock(64, kernel_size=far_kernel_size)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 12
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.attn3 = ComplexFARBlock(128, kernel_size=far_kernel_size)
        self.pool3 = ComplexAvgPool2d(2)
        
        # Block 4: 12 -> 6
        self.conv4 = ComplexConv2d(128, 256, kernel_size=3, padding=1)
        self.bn4 = ComplexBatchNorm2d(256)
        self.act4 = ModReLU(256, bias_init=-0.5)
        self.attn4 = ComplexFARBlock(256, kernel_size=far_kernel_size)
        self.pool4 = ComplexAvgPool2d(2)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = ComplexAdaptiveAvgPool2d(1)
        
        # å…¨è¿æ¥å±‚
        self.fc_in_dim = 256 * 2
        
        self.fc1 = nn.Linear(self.fc_in_dim, 256)
        self.fc2 = nn.Linear(256, 64)
        self.fc3 = nn.Linear(64, 2)
        
        self.dropout = nn.Dropout(0.4)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100]
        """
        x = x.unsqueeze(2)  # [B, 2, 1, 100, 100]
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.attn1(x)  # FAR æ³¨æ„åŠ›
        x = self.pool1(x)  # [B, 2, 32, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.attn2(x)
        x = self.pool2(x)  # [B, 2, 64, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.attn3(x)
        x = self.pool3(x)  # [B, 2, 128, 12, 12]
        
        # Block 4
        x = self.conv4(x)
        x = self.bn4(x)
        x = self.act4(x)
        x = self.attn4(x)
        x = self.pool4(x)  # [B, 2, 256, 6, 6]
        
        # å…¨å±€æ± åŒ–
        x = self.global_pool(x)  # [B, 2, 256, 1, 1]
        
        # å±•å¹³
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 512]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc3(x))
        
        return x
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_Light(nn.Module):
    """
    è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
    """
    def __init__(self):
        super().__init__()
        
        # Block 1: 100 -> 25
        self.conv1 = ComplexConv2d(1, 16, kernel_size=3, padding=1)
        self.act1 = ModReLU(16, bias_init=-0.5)
        self.pool1 = ComplexAvgPool2d(4)
        
        # Block 2: 25 -> 5
        self.conv2 = ComplexConv2d(16, 32, kernel_size=3, padding=1)
        self.act2 = ModReLU(32, bias_init=-0.5)
        self.pool2 = ComplexAvgPool2d(5)
        
        # å…¨è¿æ¥
        self.fc_in_dim = 32 * 5 * 5 * 2
        self.fc1 = nn.Linear(self.fc_in_dim, 128)
        self.fc2 = nn.Linear(128, 2)
        
    def forward(self, x):
        x = x.unsqueeze(2)
        
        x = self.pool1(self.act1(self.conv1(x)))
        x = self.pool2(self.act2(self.conv2(x)))
        
        b = x.shape[0]
        x = x.view(b, -1)
        
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        
        return x
    
    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    print("=" * 60)
    print("æµ‹è¯• FDA_CVNN æ¨¡å‹ (åŸå§‹ç‰ˆ)")
    print("=" * 60)
    
    model = FDA_CVNN()
    print(f"æ¨¡å‹å‚æ•°é‡: {model.count_parameters():,}")
    
    # æ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(4, 2, 100, 100)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        y = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y.min().item():.4f}, {y.max().item():.4f}]")
    
    # æµ‹è¯•å¸¦æ³¨æ„åŠ›çš„æ¨¡å‹
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Attention æ¨¡å‹ (SEæ³¨æ„åŠ›)")
    print("=" * 60)
    
    model_attn = FDA_CVNN_Attention(use_cbam=False)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_attn.count_parameters():,}")
    
    with torch.no_grad():
        y_attn = model_attn(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_attn.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_attn.min().item():.4f}, {y_attn.max().item():.4f}]")
    
    # æµ‹è¯• CBAM ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Attention æ¨¡å‹ (CBAMæ³¨æ„åŠ›)")
    print("=" * 60)
    
    model_cbam = FDA_CVNN_Attention(use_cbam=True)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_cbam.count_parameters():,}")
    
    with torch.no_grad():
        y_cbam = model_cbam(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_cbam.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_cbam.min().item():.4f}, {y_cbam.max().item():.4f}]")
    
    # æµ‹è¯• FAR ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_FAR æ¨¡å‹ (FARæ³¨æ„åŠ›) â­")
    print("=" * 60)
    
    model_far = FDA_CVNN_FAR(far_kernel_size=3)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_far.count_parameters():,}")
    
    with torch.no_grad():
        y_far = model_far(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_far.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_far.min().item():.4f}, {y_far.max().item():.4f}]")
    
    # æµ‹è¯•è½»é‡çº§æ¨¡å‹
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Light æ¨¡å‹")
    print("=" * 60)
    model_light = FDA_CVNN_Light()
    print(f"è½»é‡çº§æ¨¡å‹å‚æ•°é‡: {model_light.count_parameters():,}")
    
    with torch.no_grad():
        y_light = model_light(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_light.shape}")
    
    # æ¨¡å‹å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    print(f"{'æ¨¡å‹':<25} {'å‚æ•°é‡':>15} {'æ³¨æ„åŠ›ç±»å‹':<20}")
    print("-" * 60)
    print(f"{'FDA_CVNN':<25} {model.count_parameters():>15,} {'æ— ':<20}")
    print(f"{'FDA_CVNN_Attention (SE)':<25} {model_attn.count_parameters():>15,} {'é€šé“çº§ (å…¨å±€æ± åŒ–)':<20}")
    print(f"{'FDA_CVNN_Attention (CBAM)':<25} {model_cbam.count_parameters():>15,} {'é€šé“+ç©ºé—´':<20}")
    print(f"{'FDA_CVNN_FAR â­':<25} {model_far.count_parameters():>15,} {'ç©ºé—´+é€šé“ (å±€éƒ¨æ± åŒ–)':<20}")
    print(f"{'FDA_CVNN_Light':<25} {model_light.count_parameters():>15,} {'æ— ':<20}")
