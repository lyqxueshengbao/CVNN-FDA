"""
FDA-CVNN ç½‘ç»œæ¨¡å‹
ç«¯åˆ°ç«¯å›å½’ï¼šè¾“å…¥åæ–¹å·®çŸ©é˜µï¼Œè¾“å‡ºè·ç¦»å’Œè§’åº¦
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from layers_complex import (
    ComplexConv2d, 
    ComplexBatchNorm2d, 
    ModReLU, 
    ComplexAvgPool2d,
    ComplexAdaptiveAvgPool2d
)
import config as cfg


# ==========================================
# å¤æ•°æ³¨æ„åŠ›æ¨¡å—
# ==========================================
class ComplexSEBlock(nn.Module):
    """
    å¤æ•° Squeeze-and-Excitation (SE) é€šé“æ³¨æ„åŠ›
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. Squeeze: å…¨å±€å¹³å‡æ± åŒ–å‹ç¼©ç©ºé—´ç»´åº¦
    2. Excitation: ä¸¤å±‚ FC å­¦ä¹ é€šé“é—´å…³ç³»
    3. Scale: ç”¨å­¦åˆ°çš„æƒé‡é‡æ–°åŠ æƒå„é€šé“
    
    å¯¹äºå¤æ•°ï¼šä½¿ç”¨æ¨¡å€¼æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œç„¶ååŒæ—¶ç¼©æ”¾å®éƒ¨å’Œè™šéƒ¨
    """
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.avg_pool = ComplexAdaptiveAvgPool2d(1)
        
        # Excitation: ä¸¤å±‚ FC (ä½œç”¨äºæ¨¡å€¼)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        b, _, c, h, w = x.shape
        
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ– -> [B, 2, C, 1, 1]
        y = self.avg_pool(x)
        
        # è®¡ç®—æ¨¡å€¼ä½œä¸ºæ³¨æ„åŠ›è¾“å…¥ -> [B, C]
        real = y[:, 0, :, 0, 0]  # [B, C]
        imag = y[:, 1, :, 0, 0]  # [B, C]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)
        
        # Excitation: å­¦ä¹ é€šé“æƒé‡ -> [B, C]
        attn = self.fc(mag)
        
        # Scale: é‡æ–°åŠ æƒ -> [B, 1, C, 1, 1]
        attn = attn.view(b, 1, c, 1, 1)
        
        return x * attn


class ComplexFARBlock(nn.Module):
    """
    å¤æ•°ç‰ˆ FAR (Feature Attention Refinement) Block
    
    ä¸ SE çš„æ ¸å¿ƒåŒºåˆ«ï¼š
    - SE: å…¨å±€æ± åŒ– â†’ é€šé“çº§æ³¨æ„åŠ› [B, 1, C, 1, 1]
    - FAR: å±€éƒ¨æ± åŒ– â†’ ç©ºé—´+é€šé“çº§æ³¨æ„åŠ› [B, 1, C, H, W]
    
    ä¼˜åŠ¿ï¼šä¿ç•™ç©ºé—´ä½ç½®ä¿¡æ¯ï¼Œæ›´é€‚åˆåæ–¹å·®çŸ©é˜µè¿™ç§ç©ºé—´ç»“æ„æœ‰æ„ä¹‰çš„è¾“å…¥
    """
    def __init__(self, channels, kernel_size=3, reduction=4):
        super().__init__()
        
        features = max(channels // reduction, 8)  # ç¡®ä¿è‡³å°‘8ä¸ªç‰¹å¾
        padding = (kernel_size - 1) // 2
        
        # 1. å±€éƒ¨å¹³å‡æ± åŒ– (LAP) - è·å–å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä¸æ”¹å˜å°ºå¯¸
        self.local_avg_pool = ComplexAvgPool2d(
            kernel_size=kernel_size, stride=1, padding=padding
        )
        
        # 2. ç‰¹å¾é‡åŠ æƒç½‘ç»œ
        # Layer 1: é™ç»´ (1x1 Conv)
        self.conv1 = ComplexConv2d(channels, features, kernel_size=1)
        self.bn1 = ComplexBatchNorm2d(features)
        self.act1 = ModReLU(features, bias_init=-0.5)
        
        # Layer 2: å‡ç»´ (1x1 Conv)
        self.conv2 = ComplexConv2d(features, channels, kernel_size=1)
        
        # Sigmoid ç”¨äºç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        # 1. å±€éƒ¨å¹³å‡æ± åŒ–è·å–ä¸Šä¸‹æ–‡
        y = self.local_avg_pool(x)  # [B, 2, C, H, W]
        
        # 2. ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act1(y)
        y = self.conv2(y)  # [B, 2, C, H, W]
        
        # 3. åŸºäºæ¨¡å€¼ç”Ÿæˆæ³¨æ„åŠ›å›¾
        real = y[:, 0]  # [B, C, H, W]
        imag = y[:, 1]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)
        attn = self.sigmoid(mag)  # [B, C, H, W]
        
        # æ‰©å±•ç»´åº¦: [B, 1, C, H, W]
        attn = attn.unsqueeze(1)
        
        # 4. é‡åŠ æƒ
        return x * attn


class ComplexCBAM(nn.Module):
    """
    å¤æ•° CBAM (Convolutional Block Attention Module)
    = é€šé“æ³¨æ„åŠ› + ç©ºé—´æ³¨æ„åŠ›
    
    æ³¨æ„ï¼šä½¿ç”¨äº† Max Poolingï¼Œå¯èƒ½ç ´åç›¸ä½å¹²æ¶‰ç‰¹å¾
    åœ¨ä½ SNR ä¸‹ï¼Œç©ºé—´æ³¨æ„åŠ›å¯ä»¥èšç„¦äºåæ–¹å·®çŸ©é˜µä¸­çš„å…³é”®åŒºåŸŸ
    """
    def __init__(self, channels, reduction=4):
        super().__init__()
        
        # é€šé“æ³¨æ„åŠ› (SE)
        self.channel_attn = ComplexSEBlock(channels, reduction)
        
        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        """
        x: [B, 2, C, H, W]
        """
        # 1. é€šé“æ³¨æ„åŠ›
        x = self.channel_attn(x)
        
        # 2. ç©ºé—´æ³¨æ„åŠ›
        # æ²¿é€šé“ç»´åº¦å–å¹³å‡å’Œæœ€å¤§çš„æ¨¡å€¼
        real = x[:, 0]  # [B, C, H, W]
        imag = x[:, 1]
        mag = torch.sqrt(real**2 + imag**2 + 1e-8)  # [B, C, H, W]
        
        # é€šé“ç»´åº¦çš„å¹³å‡å’Œæœ€å¤§
        avg_mag = mag.mean(dim=1, keepdim=True)  # [B, 1, H, W]
        max_mag = mag.max(dim=1, keepdim=True)[0]  # [B, 1, H, W]
        
        # æ‹¼æ¥å¹¶ç”Ÿæˆç©ºé—´æ³¨æ„åŠ›å›¾
        spatial_input = torch.cat([avg_mag, max_mag], dim=1)  # [B, 2, H, W]
        spatial_attn = self.spatial_conv(spatial_input)  # [B, 1, H, W]
        spatial_attn = spatial_attn.unsqueeze(1)  # [B, 1, 1, H, W]
        
        return x * spatial_attn


class ComplexDualAttention(nn.Module):
    """
    ã€åˆ›æ–°ç‚¹æ ¸å¿ƒæ¨¡å—ã€‘ç›¸ä½ä¿æŒåŒå°ºåº¦æ³¨æ„åŠ› (Phase-Preserving Dual-Scale Attention, PP-DSA)
    
    ç»“åˆäº†ï¼š
    1. SE (Global Path): å…¨å±€å¹³å‡æ± åŒ– â†’ æ•æ‰å…¨å­”å¾„ç›¸ä½ä¾èµ–ï¼Œä¿è¯è§’åº¦åˆ†è¾¨ç‡
    2. FAR (Local Path): å±€éƒ¨å¹³å‡æ± åŒ– â†’ è½¯é˜ˆå€¼å»å™ªï¼ŒæŠ‘åˆ¶éç›¸å¹²å™ªå£°
    
    ä¼˜åŠ¿ï¼š
    - å…¨ç¨‹æ—  Max Poolingï¼Œå®Œç¾ä¿ç•™å¤æ•°ç›¸ä½çº¿æ€§å åŠ ç‰¹æ€§
    - SE æä¾›å…¨å±€é€šé“æ ¡å‡†ï¼ŒFAR æä¾›å±€éƒ¨ç©ºé—´å»å™ª
    - ä¸²è”ç»“æ„ï¼šå…ˆå…¨å±€ç»Ÿç­¹ï¼Œå†å±€éƒ¨ç²¾ä¿®
    
    ä¸ CBAM çš„åŒºåˆ«ï¼š
    - CBAM ç©ºé—´æ³¨æ„åŠ›ä½¿ç”¨ Max Poolingï¼Œå¯èƒ½ç ´åç›¸ä½å¹²æ¶‰æ¡çº¹
    - æœ¬æ¨¡å—å…¨ç¨‹ä½¿ç”¨ Average Poolingï¼Œä¿æŒç›¸ä½å®‰å…¨
    """
    def __init__(self, channels, reduction=4, far_kernel=3):
        super().__init__()
        
        # 1. å…¨å±€è·¯å¾„ (SE Block) - é€šé“çº§å…¨å±€æ ¡å‡†
        self.global_attn = ComplexSEBlock(channels, reduction)
        
        # 2. å±€éƒ¨è·¯å¾„ (FAR Block) - ç©ºé—´çº§å±€éƒ¨å»å™ª
        self.local_attn = ComplexFARBlock(channels, kernel_size=far_kernel, reduction=reduction)
        
    def forward(self, x):
        """
        ä¸²è”ç»“æ„ï¼šå…ˆå…¨å±€ç»Ÿç­¹ (SE)ï¼Œå†å±€éƒ¨ç²¾ä¿® (FAR)
        
        x: [B, 2, C, H, W]
        """
        # ç¬¬ä¸€æ­¥ï¼šå…¨å±€æ ¡å‡† (SE) - é€šé“é‡åŠ æƒ
        x = self.global_attn(x)
        
        # ç¬¬äºŒæ­¥ï¼šå±€éƒ¨å»å™ª (FAR) - ç©ºé—´+é€šé“ç²¾ä¿®
        x = self.local_attn(x)
        
        return x


class FDA_CVNN(nn.Module):
    """
    FDA-MIMO å¤æ•°å·ç§¯ç¥ç»ç½‘ç»œ
    
    è¾“å…¥: [Batch, 2, 100, 100] - åæ–¹å·®çŸ©é˜µ (å®éƒ¨é€šé“, è™šéƒ¨é€šé“)
    è¾“å‡º: [Batch, 2] - å½’ä¸€åŒ–çš„ (è·ç¦», è§’åº¦)
    
    æ¶æ„ç‰¹ç‚¹:
    1. ä½¿ç”¨å¤æ•°å·ç§¯ä¿æŒç›¸ä½ä¿¡æ¯
    2. ModReLUæ¿€æ´»å‡½æ•° (è´Ÿåç½®åˆ›é€ éçº¿æ€§)
    3. å¹³å‡æ± åŒ– (ä¸ç ´åç›¸ä½)
    """
    def __init__(self):
        super().__init__()
        
        # è¾“å…¥: [B, 2, 1, 100, 100] -> éœ€è¦è°ƒæ•´ä¸º [B, 2, 1, H, W]
        # é€šé“æ•°ç¿»å€: 32 -> 64 -> 128ï¼Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 5
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.pool3 = ComplexAvgPool2d(5)
        
        # å…¨è¿æ¥å±‚
        # ç‰¹å¾å›¾å¤§å°: 5x5, é€šé“128, å®éƒ¨+è™šéƒ¨
        self.fc_in_dim = 128 * 5 * 5 * 2  # 6400
        
        self.fc1 = nn.Linear(self.fc_in_dim, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 2)  # è¾“å‡º r å’Œ theta
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100] - å®éƒ¨å’Œè™šéƒ¨
        """
        # è°ƒæ•´ç»´åº¦: [B, 2, H, W] -> [B, 2, 1, H, W]
        x = x.unsqueeze(2)
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.pool1(x)  # [B, 2, 16, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.pool2(x)  # [B, 2, 32, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.pool3(x)  # [B, 2, 64, 5, 5]
        
        # å±•å¹³: å°†å¤æ•°ç»´åº¦å’Œç©ºé—´ç»´åº¦åˆå¹¶
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 2*64*5*5]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        return x
    
    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_Attention(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ FDA-CVNN
    
    è®¾è®¡åŸåˆ™ï¼šå®Œå…¨ä¿æŒåŸå§‹ FDA_CVNN çš„æ¶æ„ï¼ŒåªåŠ å…¥è½»é‡çº§æ³¨æ„åŠ›æ¨¡å—
    - ä¿æŒ 3 å±‚å·ç§¯ç»“æ„ï¼ˆä¸åŠ æ·±ï¼‰
    - ä¿æŒ pool3(5) è¾“å‡º 5x5 ç‰¹å¾å›¾ï¼ˆä¸ä½¿ç”¨å…¨å±€æ± åŒ–ï¼‰
    - ä¿æŒç›¸åŒçš„å…¨è¿æ¥å±‚ç»“æ„
    
    å‚æ•°:
        attention_type: æ³¨æ„åŠ›ç±»å‹
            - 'se': SE é€šé“æ³¨æ„åŠ› (å…¨å±€å¹³å‡æ± åŒ–)
            - 'cbam': CBAM (SE + ç©ºé—´æ³¨æ„åŠ›ï¼Œå« MaxPoolï¼Œå¯èƒ½ç ´åç›¸ä½)
            - 'far': FAR å±€éƒ¨æ³¨æ„åŠ› (å±€éƒ¨å¹³å‡æ± åŒ–)
            - 'dual': ã€åˆ›æ–°ã€‘SE + FAR ä¸²è” (ç›¸ä½ä¿æŒåŒå°ºåº¦æ³¨æ„åŠ› PP-DSA)
        se_reduction: æ³¨æ„åŠ›æ¨¡å—çš„é€šé“å‹ç¼©æ¯”ï¼Œé»˜è®¤ 4
        deep_only: æ˜¯å¦åªåœ¨æ·±å±‚ä½¿ç”¨æ³¨æ„åŠ› (Block2, Block3)ï¼Œé»˜è®¤ False
        far_kernel: FAR å±€éƒ¨æ± åŒ–æ ¸å¤§å°ï¼Œé»˜è®¤ 3
    """
    def __init__(self, attention_type='se', se_reduction=4, deep_only=False, far_kernel=3,
                 use_cbam=False):  # use_cbam ä¿ç•™ç”¨äºå‘åå…¼å®¹
        super().__init__()
        
        # å‘åå…¼å®¹ï¼šå¦‚æœä½¿ç”¨æ—§çš„ use_cbam å‚æ•°
        if use_cbam:
            attention_type = 'cbam'
        
        self.attention_type = attention_type
        self.se_reduction = se_reduction
        self.deep_only = deep_only
        
        # å®šä¹‰æ³¨æ„åŠ›æ„å»ºå‡½æ•°
        def build_attn(channels):
            if attention_type == 'cbam':
                return ComplexCBAM(channels, reduction=se_reduction)
            elif attention_type == 'far':
                return ComplexFARBlock(channels, kernel_size=far_kernel, reduction=se_reduction)
            elif attention_type == 'dual':
                return ComplexDualAttention(channels, reduction=se_reduction, far_kernel=far_kernel)
            else:  # 'se' æˆ–é»˜è®¤
                return ComplexSEBlock(channels, reduction=se_reduction)
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        # æµ…å±‚æ³¨æ„åŠ›å¯é€‰
        if not deep_only:
            self.attn1 = build_attn(32)
        else:
            self.attn1 = None
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.attn2 = build_attn(64)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 5
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.attn3 = build_attn(128)
        self.pool3 = ComplexAvgPool2d(5)  # è¾“å‡º 5x5
        
        # å…¨è¿æ¥å±‚
        self.fc_in_dim = 128 * 5 * 5 * 2  # 6400
        
        self.fc1 = nn.Linear(self.fc_in_dim, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 2)
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100]
        """
        x = x.unsqueeze(2)  # [B, 2, 1, 100, 100]
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        if self.attn1 is not None:
            x = self.attn1(x)
        x = self.pool1(x)  # [B, 2, 32, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.attn2(x)
        x = self.pool2(x)  # [B, 2, 64, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.attn3(x)
        x = self.pool3(x)  # [B, 2, 128, 5, 5]
        
        # å±•å¹³
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 6400]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        
        return x
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_FAR(nn.Module):
    """
    ä¿å®ˆç‰ˆ FAR æ³¨æ„åŠ› FDA-CVNN
    
    FAR vs SE çš„åŒºåˆ«ï¼š
    - SE: å…¨å±€æ± åŒ– â†’ é€šé“çº§æƒé‡ [B, 1, C, 1, 1]ï¼ˆæ‰€æœ‰ä½ç½®ç›¸åŒæƒé‡ï¼‰
    - FAR: å±€éƒ¨æ± åŒ– â†’ ç©ºé—´+é€šé“çº§æƒé‡ [B, 1, C, H, W]ï¼ˆä¸åŒä½ç½®ä¸åŒæƒé‡ï¼‰
    
    è®¾è®¡åŸåˆ™ï¼šä¿æŒåŸå§‹æ¶æ„ï¼Œåªæ›¿æ¢æ³¨æ„åŠ›æ¨¡å—
    """
    def __init__(self, far_kernel_size=3):
        super().__init__()
        
        # Block 1: 100 -> 50
        self.conv1 = ComplexConv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = ComplexBatchNorm2d(32)
        self.act1 = ModReLU(32, bias_init=-0.5)
        self.attn1 = ComplexFARBlock(32, kernel_size=far_kernel_size)
        self.pool1 = ComplexAvgPool2d(2)
        
        # Block 2: 50 -> 25
        self.conv2 = ComplexConv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = ComplexBatchNorm2d(64)
        self.act2 = ModReLU(64, bias_init=-0.5)
        self.attn2 = ComplexFARBlock(64, kernel_size=far_kernel_size)
        self.pool2 = ComplexAvgPool2d(2)
        
        # Block 3: 25 -> 5
        self.conv3 = ComplexConv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = ComplexBatchNorm2d(128)
        self.act3 = ModReLU(128, bias_init=-0.5)
        self.attn3 = ComplexFARBlock(128, kernel_size=far_kernel_size)
        self.pool3 = ComplexAvgPool2d(5)  # è¾“å‡º 5x5
        
        # å…¨è¿æ¥å±‚ (ä¸åŸå§‹ä¸€è‡´)
        self.fc_in_dim = 128 * 5 * 5 * 2  # 6400
        
        self.fc1 = nn.Linear(self.fc_in_dim, 512)
        self.fc2 = nn.Linear(512, 128)
        self.fc3 = nn.Linear(128, 2)
        
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        """
        x: [B, 2, 100, 100]
        """
        x = x.unsqueeze(2)  # [B, 2, 1, 100, 100]
        
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.act1(x)
        x = self.attn1(x)  # FAR æ³¨æ„åŠ›
        x = self.pool1(x)  # [B, 2, 32, 50, 50]
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.act2(x)
        x = self.attn2(x)
        x = self.pool2(x)  # [B, 2, 64, 25, 25]
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.act3(x)
        x = self.attn3(x)
        x = self.pool3(x)  # [B, 2, 128, 5, 5]
        
        # å±•å¹³
        b = x.shape[0]
        x = x.view(b, -1)  # [B, 6400]
        
        # å…¨è¿æ¥å›å½’
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = torch.sigmoid(self.fc3(x))
        
        return x
    
    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class FDA_CVNN_Light(nn.Module):
    """
    è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
    """
    def __init__(self):
        super().__init__()
        
        # Block 1: 100 -> 25
        self.conv1 = ComplexConv2d(1, 16, kernel_size=3, padding=1)
        self.act1 = ModReLU(16, bias_init=-0.5)
        self.pool1 = ComplexAvgPool2d(4)
        
        # Block 2: 25 -> 5
        self.conv2 = ComplexConv2d(16, 32, kernel_size=3, padding=1)
        self.act2 = ModReLU(32, bias_init=-0.5)
        self.pool2 = ComplexAvgPool2d(5)
        
        # å…¨è¿æ¥
        self.fc_in_dim = 32 * 5 * 5 * 2
        self.fc1 = nn.Linear(self.fc_in_dim, 128)
        self.fc2 = nn.Linear(128, 2)
        
    def forward(self, x):
        x = x.unsqueeze(2)
        
        x = self.pool1(self.act1(self.conv1(x)))
        x = self.pool2(self.act2(self.conv2(x)))
        
        b = x.shape[0]
        x = x.view(b, -1)
        
        x = F.relu(self.fc1(x))
        x = torch.sigmoid(self.fc2(x))
        
        return x
    
    def count_parameters(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    print("=" * 60)
    print("æµ‹è¯• FDA_CVNN æ¨¡å‹ (åŸå§‹ç‰ˆ)")
    print("=" * 60)
    
    model = FDA_CVNN()
    print(f"æ¨¡å‹å‚æ•°é‡: {model.count_parameters():,}")
    
    # æ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(4, 2, 100, 100)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        y = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y.min().item():.4f}, {y.max().item():.4f}]")
    
    # æµ‹è¯•å¸¦æ³¨æ„åŠ›çš„æ¨¡å‹
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Attention æ¨¡å‹ (SEæ³¨æ„åŠ›)")
    print("=" * 60)
    
    model_attn = FDA_CVNN_Attention(use_cbam=False)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_attn.count_parameters():,}")
    
    with torch.no_grad():
        y_attn = model_attn(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_attn.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_attn.min().item():.4f}, {y_attn.max().item():.4f}]")
    
    # æµ‹è¯• CBAM ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Attention æ¨¡å‹ (CBAMæ³¨æ„åŠ›)")
    print("=" * 60)
    
    model_cbam = FDA_CVNN_Attention(use_cbam=True)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_cbam.count_parameters():,}")
    
    with torch.no_grad():
        y_cbam = model_cbam(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_cbam.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_cbam.min().item():.4f}, {y_cbam.max().item():.4f}]")
    
    # æµ‹è¯• FAR ç‰ˆæœ¬
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_FAR æ¨¡å‹ (FARæ³¨æ„åŠ›) â­")
    print("=" * 60)
    
    model_far = FDA_CVNN_FAR(far_kernel_size=3)
    print(f"æ¨¡å‹å‚æ•°é‡: {model_far.count_parameters():,}")
    
    with torch.no_grad():
        y_far = model_far(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_far.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{y_far.min().item():.4f}, {y_far.max().item():.4f}]")
    
    # æµ‹è¯•è½»é‡çº§æ¨¡å‹
    print("\n" + "=" * 60)
    print("æµ‹è¯• FDA_CVNN_Light æ¨¡å‹")
    print("=" * 60)
    model_light = FDA_CVNN_Light()
    print(f"è½»é‡çº§æ¨¡å‹å‚æ•°é‡: {model_light.count_parameters():,}")
    
    with torch.no_grad():
        y_light = model_light(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {y_light.shape}")
    
    # æ¨¡å‹å¯¹æ¯”æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”æ€»ç»“")
    print("=" * 60)
    print(f"{'æ¨¡å‹':<25} {'å‚æ•°é‡':>15} {'æ³¨æ„åŠ›ç±»å‹':<20}")
    print("-" * 60)
    print(f"{'FDA_CVNN':<25} {model.count_parameters():>15,} {'æ— ':<20}")
    print(f"{'FDA_CVNN_Attention (SE)':<25} {model_attn.count_parameters():>15,} {'é€šé“çº§ (å…¨å±€æ± åŒ–)':<20}")
    print(f"{'FDA_CVNN_Attention (CBAM)':<25} {model_cbam.count_parameters():>15,} {'é€šé“+ç©ºé—´':<20}")
    print(f"{'FDA_CVNN_FAR â­':<25} {model_far.count_parameters():>15,} {'ç©ºé—´+é€šé“ (å±€éƒ¨æ± åŒ–)':<20}")
    print(f"{'FDA_CVNN_Light':<25} {model_light.count_parameters():>15,} {'æ— ':<20}")
